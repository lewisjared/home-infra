---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: k8s-monitoring
  namespace: monitoring
spec:
  interval: 30m
  timeout: 15m
  # Wait for destinations to be ready before deploying Alloy
  dependsOn:
    - name: kube-prometheus-stack
    - name: loki
    - name: tempo
  chart:
    spec:
      chart: k8s-monitoring
      version: "3.5.6"
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: monitoring
      interval: 12h
  install:
    createNamespace: false
    remediation:
      retries: 3
  upgrade:
    force: true
    remediation:
      retries: 3
      remediateLastFailure: true
    cleanupOnFail: true
  uninstall:
    keepHistory: false
  rollback:
    recreate: true
    cleanupOnFail: true
  values:
    # Cluster identification
    cluster:
      name: home-infra

    # Destination configuration for self-hosted LGTM stack
    destinations:
      - name: local-prometheus
        type: prometheus
        url: http://prometheus-prometheus.monitoring.svc.cluster.local:9090/api/v1/write
        metrics: { enabled: true }

      - name: local-loki
        type: loki
        url: http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push
        logs: { enabled: true }

      - name: local-tempo
        type: otlp
        protocol: grpc
        url: http://tempo.monitoring.svc.cluster.local:4317
        # Only enable traces for tempo
        metrics: { enabled: false }
        logs: { enabled: false }
        traces: { enabled: true }

    # Feature configuration
    clusterMetrics:
      enabled: true
      destinations:
        - local-prometheus
      node-exporter:
        # Disable the default allow list so all node_* metrics are collected
        # (the default only keeps a small subset for Grafana Cloud)
        metricsTuning:
          useDefaultAllowList: false
        # Enable PSI (Pressure Stall Information) metrics
        extraArgs:
          - --collector.pressure
        # Relabel job to "node-exporter" to match kube-prometheus-stack
        # recording rules and standard Grafana dashboards
        extraMetricProcessingRules: |-
          rule {
            action = "replace"
            replacement = "node-exporter"
            target_label = "job"
          }

    clusterEvents:
      enabled: true
      destinations:
        - local-loki

    podLogs:
      enabled: true
      destinations:
        - local-loki

    nodeLogs:
      enabled: true
      destinations:
        - local-loki

    integrations:
      loki:
        instances:
          - name: loki
            labelSelectors:
              app.kubernetes.io/name: loki
            # Don't collect logs as we are forwarding logs to loki directly
            logs:
              enabled: false
            metrics:
              enabled: true
      grafana:
        instances:
          - name: grafana
            labelSelectors:
              app.kubernetes.io/name: grafana

    # Application observability (for app instrumentation)
    applicationObservability:
      enabled: true
      destinations:
        - local-loki
        - local-tempo
        - local-prometheus
      receivers:
        otlp:
          grpc:
            enabled: true
            port: 4317
          http:
            enabled: true
            port: 4318

    # Alloy configuration
    alloy-logs:
      enabled: true

      # Mount audit log directory from host
      extraVolumes:
        - name: audit-logs
          hostPath:
            path: /var/log/kubernetes/audit
            type: DirectoryOrCreate

      extraVolumeMounts:
        - name: audit-logs
          mountPath: /var/log/kubernetes/audit
          readOnly: true

      # Custom configuration for Kubernetes audit log collection
      extraConfig: |
        // Kubernetes Audit Log Collection
        // Collect audit logs from control plane nodes
        local.file_match "kubernetes_audit" {
          path_targets = [{
            "__path__"  = "/var/log/kubernetes/audit/*.log",
            "job"       = "kubernetes-audit",
            "node_name" = env("HOSTNAME"),
            "cluster"   = "home-infra",
          }]
          sync_period = "5s"
        }

        loki.source.file "kubernetes_audit" {
          targets    = local.file_match.kubernetes_audit.targets
          forward_to = [loki.process.kubernetes_audit.receiver]
          tail_from_end = true
        }

        loki.process "kubernetes_audit" {
          // Parse JSON audit log format
          stage.json {
            expressions = {
              "level"       = "level",
              "verb"        = "verb",
              "user"        = "user.username",
              "namespace"   = "objectRef.namespace",
              "resource"    = "objectRef.resource",
              "name"        = "objectRef.name",
              "stage"       = "stage",
              "status_code" = "responseStatus.code",
              "api_group"   = "objectRef.apiGroup",
            }
          }

          // Add extracted fields as labels for efficient querying
          stage.labels {
            values = {
              "level"       = "level",
              "verb"        = "verb",
              "user"        = "user",
              "namespace"   = "namespace",
              "resource"    = "resource",
              "stage"       = "stage",
              "status_code" = "status_code",
              "api_group"   = "api_group",
            }
          }

          // Forward to Loki (uses chart's auto-generated destination)
          forward_to = [loki.write.local_loki.receiver]
        }

    alloy-metrics:
      enabled: true

    alloy-singleton:
      enabled: true

    alloy-receiver:
      enabled: true
