---
apiVersion: source.toolkit.fluxcd.io/v1
kind: OCIRepository
metadata:
  name: climate-ref
  namespace: climate-ref
spec:
  interval: 1h
  url: oci://ghcr.io/climate-ref/charts/ref
  ref:
    tag: 0.11.1
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: climate-ref
  namespace: climate-ref
spec:
  releaseName: climate-ref
  interval: 30m
  timeout: 10m
  chartRef:
    kind: OCIRepository
    name: climate-ref
    namespace: climate-ref
  install:
    createNamespace: false
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
      remediateLastFailure: true
    cleanupOnFail: true
  rollback:
    recreate: true
    cleanupOnFail: true
  values:
    # Global image configuration
    defaults:
      image:
        repository: ghcr.io/climate-ref/climate-ref
        tag: v0.11.1
      env:
        COLUMNS: "160"
        REF_CONFIGURATION: /ref/test-lazy
        REF_SOFTWARE_ROOT: "/ref/software"
        REF_CMIP6_PARSER: drs
        CONDA_PKGS_DIRS: /tmp/conda-pkgs
        XDG_CONFIG_HOME: /home/app/.config
      serviceAccount:
        create: true
      podSecurityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        capabilities:
          drop:
            - ALL
      # Persistent NFS volumes for CMIP6 data, observations, and REF state
      volumes:
        - name: ref-data
          persistentVolumeClaim:
            claimName: climate-ref-state
        - name: cmip6-data
          persistentVolumeClaim:
            claimName: climate-ref-cmip6
        - name: obs-data
          persistentVolumeClaim:
            claimName: climate-ref-obs
        - name: config
          emptyDir: {}
        - name: tmp
          emptyDir: {}
        - name: esmvaltool-config
          configMap:
            name: climate-ref-esmvaltool-config
      volumeMounts:
        - name: ref-data
          mountPath: /ref
        - name: cmip6-data
          mountPath: /data/cmip6
        - name: obs-data
          mountPath: /data/obs
        - name: config
          mountPath: /home/app/.config
        - name: tmp
          mountPath: /tmp
        - name: esmvaltool-config
          mountPath: /home/app/.config/esmvaltool/config.yaml
          subPath: config.yaml
          readOnly: true

    # Flower (Celery monitoring UI) configuration
    flower:
      enabled: true
      image:
        repository: mher/flower
        tag: "2.0.1@sha256:51c3c3db5be343e6335e3dbca5348708932bbcca992df6a39f1dad91e3b340df"
      replicaCount: 1
      env:
        FLOWER_TASK_RUNTIME_METRIC_BUCKETS: "30,60,120,180,300,450,600,900,1200,1800"
      service:
        type: ClusterIP
        port: 5555
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      containerSecurityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        capabilities:
          drop:
            - ALL

    # Dragonfly configuration (Celery broker and result backend)
    dragonfly:
      enabled: true
      storage:
        enabled: true
      extraArgs:
        - --proactor_threads=1
        - --maxmemory=1Gi

    # Provider configurations
    providers:
      orchestrator: {}
      esmvaltool:
        replicaCount: 4
        # autoscaling:
        #   enabled: false
        #   minReplicas: 1
        #   maxReplicas: 10
        resources:
          requests:
            memory: 8Gi
            cpu: "4"
        env:
          CELERY_WORKER_CONCURRENCY: "2"
      pmp:
        replicaCount: 4
        # autoscaling:
        #   enabled: false
        #   minReplicas: 1
        #   maxReplicas: 10
        resources:
          requests:
            memory: 8Gi
            cpu: "4"
        env:
          CELERY_WORKER_CONCURRENCY: "2"
          # Use synchronous Dask scheduler to avoid crashes from excessive
          # parallelism when running multiple executions on the same machine.
          # See: https://github.com/Climate-REF/climate-ref/issues/437
          DASK_SCHEDULER: synchronous
      ilamb:
        replicaCount: 2
        # autoscaling:
        #   enabled: false
        #   minReplicas: 1
        #   maxReplicas: 10
        resources:
          requests:
            memory: 8Gi
            cpu: "4"
        env:
          CELERY_WORKER_CONCURRENCY: "2"
          # Use synchronous Dask scheduler to avoid crashes from excessive
          # parallelism when running multiple executions on the same machine.
          # See: https://github.com/Climate-REF/climate-ref/issues/437
          DASK_SCHEDULER: synchronous
